# 1 = Climate Change
# 2 = Climate Manipulation
# 3 = Global Warming
fold <- folder[3]
# get list of comment files
listfiles <- list.files(fold, pattern = "comments") %>% as.vector()
listfiles
# get list of comment files
listfiles <- list.files(fold, pattern = "comments") %>%
as.vector()
dfcomm <- paste(fold,listfiles[1],sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "")
View(dfcomm)
dfcomm <- paste(fold,listfiles[1],sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = 1)
view(dfcomm)
split(listfiles[1])
strsplit(listfiles[1])
strsplit(listfiles[1], split = "_")
strsplit(listfiles[1], split = "_")[2]
strsplit(listfiles[1], split = "_")[[2]]
strsplit(listfiles[1], split = "_")[1][2]
strsplit(listfiles[1], split = "_")
videoid <- strsplit(listfiles[1], split = "_")
select(videoid)
videoid[2]
videoid[1]
videoid[[1][2]]
videoid[[1]][2]
videoid <- strsplit(listfiles, split = "_")
videoid[[1]][2]
videoid
dfcomm <- data.frame()
# iterate, read and append all files in "dfcm"
for (val in listfiles) {
commtemp <- paste(fold,val,sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[val]][2])
dfcomm <- rbind(dfcomm,commtemp)
print(val)
}
videoid <- strsplit(listfiles, split = "_")
dfcomm <- paste(fold,listfiles[1],sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[1]][2])
View(dfcomm)
listfiles <- listfiles[-1]
videoid <- videoid[-1]
# iterate, read and append all files in "dfcm"
for (val in listfiles) {
commtemp <- paste(fold,val,sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[val]][2])
dfcomm <- rbind(dfcomm,commtemp)
print(val)
}
commtemp <- paste(fold,val,sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "")
commtemp <- paste(fold,val,sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[val]][2])
commtemp <- paste(fold,val,sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
video = videoid[[val]][2]
commtemp <- paste(fold,val,sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[1]][2])
commtemp
dfcomm <- rbind(dfcomm,commtemp)
print(val)
dfcomm <- paste(fold,listfiles[1],sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[1]][2])
listfiles <- listfiles[-1]
videoid <- videoid[-1]
# iterate, read and append all files in "dfcm"
for (val in 1:length(listfiles)) {
commtemp <- paste(fold,listfiles[val],sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[val]][2])
dfcomm <- rbind(dfcomm,commtemp)
print(val)
}
View(dfcomm)
wddf <- dfcomm %>%
select(text, video) %>%
as.data.frame() %>%
mutate(text = as.character(text))
source('C:/Users/fcalv/Desktop/R folder/Climatedream/Avaaz Comm/CommScript.R', echo=TRUE)
library(rstudioapi)
library(tidyverse)
library(dplyr)
library(rvest)
library(rlist)
library(tidytext)
source('transcriptTED.R')
setwd(dirname(getSourceEditorContext()$path))
# Disable text encoding as 'factors'
options(stringsAsFactors = FALSE)
# URL of the playlists
#"https://www.ted.com/playlists/126/the_big_picture",
#"https://www.ted.com/playlists/78/climate_change_oh_it_s_real",
#"https://www.ted.com/playlists/154/how_do_you_solve_a_problem_lik",
#"https://www.ted.com/playlists/493/why_climate_change_is_a_human",
#"https://www.ted.com/playlists/634/a_day_trip_to_antarctica",
#"https://www.ted.com/playlists/439/what_is_the_anthropocene",
#"https://www.ted.com/playlists/151/earth_appreciated",
#"https://www.ted.com/playlists/142/the_forecast_calls_for"
playlisturl <- c("https://www.ted.com/playlists/126/the_big_picture",
"https://www.ted.com/playlists/78/climate_change_oh_it_s_real",
"https://www.ted.com/playlists/154/how_do_you_solve_a_problem_lik",
"https://www.ted.com/playlists/493/why_climate_change_is_a_human",
"https://www.ted.com/playlists/634/a_day_trip_to_antarctica",
"https://www.ted.com/playlists/439/what_is_the_anthropocene",
"https://www.ted.com/playlists/151/earth_appreciated",
"https://www.ted.com/playlists/142/the_forecast_calls_for")
fulllist <- vector()
for (val in 1:length(playlisturl)) {
htmlpage <- playlisturl[val] %>% read_html()
htmlpage %>% write_html(paste("playlist", as.character(val),".html",sep = ""))
# scrape list of videos from playlist
videolist <- htmlpage %>% html_nodes('body') %>%
html_nodes('a') %>%
html_attr('href') %>%
as.vector() %>%
gsub(pattern = '\\?(.*)', replacement = "")
# clean list of video
videolist <- videolist[grep(x = videolist, pattern = '/talks/')]
videolist <- videolist[-1]
videolist <- paste('https://www.ted.com/', videolist, '/transcript', sep = "")
fulllist <- append(fulllist,videolist)
}
fulllist <- unique(fulllist)
trnslist <- vector()
for (val in fulllist) {
temp <- val %>% transcriptTED()
trnslist <- trnslist %>% list.append(temp)
}
strn <- "TED.com translations are made possible by volunteer translators. Learn more about the Open Translation Project."
trnslist <- trnslist[-grep(pattern = strn, x = trnslist)]
strn <- "© TED Conferences, LLC. All rights reserved."
trnslist <- trnslist %>% gsub(pattern = strn,replacement = "©")
trnslist <- split(trnslist, cumsum(trnslist == "©"))
trnslist <- trnslist[-length(trnslist)]
# trnslist %>% saveRDS(file = "texttranscription.rds")
trnslist %>% glimpse
dfspeech <- data.frame()
for (val in 1:length(trnslist)) {
dfspeech <- rbind(dfspeech, trnslist[val] %>% as.data.frame(col.names = "text") %>% cbind(speech = as.character(val)))
print(val)
}
dfwords <- dfspeech %>%
unnest_tokens(word, text) %>%
count(speech, word, sort = T)
total_words <- dfwords %>%
group_by(speech) %>%
summarize(total = sum(n))
dfwords <- left_join(dfwords, total_words)
dfwords <- dfwords %>%
bind_tf_idf(word, speech, n)
dfwords %>%
arrange(desc(tf_idf))
# same with bigrams
dfbigrams <- dfspeech %>%
unnest_tokens(bigram, text, token = 'ngrams', n =2)
# count
# dfbigrams %>%
# count(bigram, sort = T)
# filter stopwords
dfbisep <- dfbigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
dfbifiltered <- dfbisep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
dfbifiltered %>%
count(word1, word2, sort = T)
library(rstudioapi)
library(tidyverse)
library(dplyr)
library(rvest)
library(rlist)
library(tidytext)
source('transcriptTED.R')
setwd(dirname(getSourceEditorContext()$path))
# Disable text encoding as 'factors'
options(stringsAsFactors = FALSE)
# URL of the playlists
#"https://www.ted.com/playlists/126/the_big_picture",
#"https://www.ted.com/playlists/78/climate_change_oh_it_s_real",
#"https://www.ted.com/playlists/154/how_do_you_solve_a_problem_lik",
#"https://www.ted.com/playlists/493/why_climate_change_is_a_human",
#"https://www.ted.com/playlists/634/a_day_trip_to_antarctica",
#"https://www.ted.com/playlists/439/what_is_the_anthropocene",
#"https://www.ted.com/playlists/151/earth_appreciated",
#"https://www.ted.com/playlists/142/the_forecast_calls_for"
playlisturl <- c("https://www.ted.com/playlists/126/the_big_picture",
"https://www.ted.com/playlists/78/climate_change_oh_it_s_real",
"https://www.ted.com/playlists/154/how_do_you_solve_a_problem_lik",
"https://www.ted.com/playlists/493/why_climate_change_is_a_human",
"https://www.ted.com/playlists/634/a_day_trip_to_antarctica",
"https://www.ted.com/playlists/439/what_is_the_anthropocene",
"https://www.ted.com/playlists/151/earth_appreciated",
"https://www.ted.com/playlists/142/the_forecast_calls_for")
fulllist <- vector()
for (val in 1:length(playlisturl)) {
htmlpage <- playlisturl[val] %>% read_html()
htmlpage %>% write_html(paste("playlist", as.character(val),".html",sep = ""))
# scrape list of videos from playlist
videolist <- htmlpage %>% html_nodes('body') %>%
html_nodes('a') %>%
html_attr('href') %>%
as.vector() %>%
gsub(pattern = '\\?(.*)', replacement = "")
# clean list of video
videolist <- videolist[grep(x = videolist, pattern = '/talks/')]
videolist <- videolist[-1]
videolist <- paste('https://www.ted.com/', videolist, '/transcript', sep = "")
fulllist <- append(fulllist,videolist)
}
fulllist <- unique(fulllist)
trnslist <- vector()
for (val in fulllist) {
temp <- val %>% transcriptTED()
trnslist <- trnslist %>% list.append(temp)
}
strn <- "TED.com translations are made possible by volunteer translators. Learn more about the Open Translation Project."
trnslist <- trnslist[-grep(pattern = strn, x = trnslist)]
strn <- "© TED Conferences, LLC. All rights reserved."
trnslist <- trnslist %>% gsub(pattern = strn,replacement = "©")
trnslist <- split(trnslist, cumsum(trnslist == "©"))
trnslist <- trnslist[-length(trnslist)]
# trnslist %>% saveRDS(file = "texttranscription.rds")
trnslist %>% glimpse
dfspeech <- data.frame()
for (val in 1:length(trnslist)) {
dfspeech <- rbind(dfspeech, trnslist[val] %>% as.data.frame(col.names = "text") %>% cbind(speech = as.character(val)))
print(val)
}
dfwords <- dfspeech %>%
unnest_tokens(word, text) %>%
count(speech, word, sort = T)
total_words <- dfwords %>%
group_by(speech) %>%
summarize(total = sum(n))
dfwords <- left_join(dfwords, total_words)
dfwords <- dfwords %>%
bind_tf_idf(word, speech, n)
dfwords %>%
arrange(desc(tf_idf))
# same with bigrams
dfbigrams <- dfspeech %>%
unnest_tokens(bigram, text, token = 'ngrams', n =2)
dfbigrams <- dfspeech %>%
unnest_tokens(bigram, text, token = 'ngrams', n =2)
# count
dfbigrams %>%
count(bigram, sort = T)
dfbisep <- dfbigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
dfbifiltered <- dfbisep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
dfbifiltered %>%
count(word1, word2, sort = T)
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write.csv2(file = "bigramsfrequency.txt",sep = "\t")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write.csv2(file = "bigramsfrequency.txt",sep = "\t")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write_file(file = "bigramsfrequency.txt",sep = "\t")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write_csv(file = "bigramsfrequency.txt",sep = "\t")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write_csv("bigramsfrequency.txt",sep = "\t")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write_csv("bigramsfrequency.txt",sep = ";")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write("bigramsfrequency.csv")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write("bigramsfrequency.csv",sep = "\t")
write(x = dfbifiltered,file = "bigramsfrequency.csv",sep = "\t")
dfbifiltered <- dfbifiltered %>%
count(word1, word2, sort = T)
write(x = dfbifiltered,file = "bigramsfrequency.csv",sep = "\t")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
as.data.frame() %>%
write(file = "bigramsfrequency.csv",sep = "\t")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write.csv(file = "bigramsfrequency.csv")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write(file = "bigramsfrequency.csv",sep = "\t")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write(file = "bigramsfrequency.csv")
dfbifiltered <- dfbisep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write(file = "bigramsfrequency.csv")
dfbifiltered <- dfbisep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
dfbifiltered %>%
count(word1, word2, sort = T) %>%
write.csv(file = "bigramsfrequency.csv")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
unite(col = bigram,data = word1,word2, sep = " ")
dfbifiltered %>%
count(word1, word2, sort = T) %>%
unite(col = "bigram", word1,word2, sep = " ") %>%
write.csv(file = "bigramsfrequency.csv")
bigr_tf_idf <- dfbigrams %>%
count(speech, bigram) %>%
bind_tf_idf(bigram, speech, n) %>%
arrange(desc(tf_idf))
bigr_tf_idf <- dfbigrams %>%
count(speech, bigram) %>%
bind_tf_idf(bigram, speech, n) %>%
arrange(desc(tf_idf))
bigr_tf_idf
bigr_tf_idf %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
dfwords %>%
arrange(desc(tf_idf))
dfwords %>%
arrange(desc(tf_idf)) %>%
write.csv("unigramtf_idf.csv")
source('C:/Users/fcalv/Desktop/R folder/tian/Climatedream/TED/script.R', echo=TRUE)
source('C:/Users/fcalv/Desktop/R folder/tian/Climatedream/TED/script.R', echo=TRUE)
glimpse(wddf_bigrams)
library(rstudioapi)
library(tidyverse)
library(dplyr)
library(tidytext)
library(igraph)
library(ggraph)
setwd(dirname(getSourceEditorContext()$path))
# Disable text encoding as 'factors'
options(stringsAsFactors = FALSE)
# folder to use
folder <- c("Scraped ClimateChange","Scraped ClimateManipulation","Scraped GlobalWarming")
# 1 = Climate Change
# 2 = Climate Manipulation
# 3 = Global Warming
fold <- folder[3]
###########
# READ FILES
# get list of comment files
listfiles <- list.files(fold, pattern = "comments") %>%
as.vector()
videoid <- strsplit(listfiles, split = "_")
dfcomm <- paste(fold,listfiles[1],sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[1]][2])
listfilest <- listfiles[-1]
videoid <- videoid[-1]
# iterate, read and append all files in "dfcm"
for (val in 1:length(listfilest)) {
commtemp <- paste(fold,listfilest[val],sep = "/") %>%
read.table(sep = '\t', header = T, fill = T, quote = "") %>%
cbind(video = videoid[[val]][2])
dfcomm <- rbind(dfcomm,commtemp)
print(val)
}
# DON'T RUN
# print entire list of all comments in "commfull.csv"
# commf %>% write.csv("commfull.csv")
# get list of authors files
listfiles <- list.files(fold, pattern = "authors") %>% as.vector()
dfauth <- paste(fold,listfiles[1],sep = "/") %>% read.table(sep = '\t', header = F, fill = T, quote = "")
listfiles <- listfiles[-1]
# iterate, read and append all files in "dfauth"
for (val in listfiles) {
authtemp <- paste(fold,val,sep = "/") %>% read.table(sep = '\t', header = F, fill = T, quote = "")
dfauth <- rbind(dfauth,authtemp)
print(val)
}
dfauth <- dfauth %>% rename("Author"="V1", "Commnum"="V2")
# Summarize by comment author across all videos and order
dfauth <- dfauth %>% group_by(Author) %>% summarise(Commnum = sum(Commnum))
dfauth <- dfauth[order(-dfauth$Commnum),]
# Top N comment authors file "TopN users commenting.csv"
N <- 100
dfauth %>% head(N) %>% write.csv(paste(fold,"TopN users commenting.csv",sep = "/"))
############
# Word frequency in comments
# data frame
wddf <- dfcomm %>%
select(text, video) %>%
as.data.frame() %>%
mutate(text = as.character(text))
# Tidy text
wddfbycomm <- wddf %>% tibble(text = ., comnum = 1:nrow(wddf))
wddfti <- wddf %>% unnest_tokens(word, text)
# Word frequency unfiltered
wddfti <- wddfti %>%
count(word, sort = T)
# print unfiltered result
wddfti %>%
write_csv(paste(fold,"wordfreq.csv",sep = "/"))
# Anti join stop words see help(stop_words)
data(stop_words)
wddftif <- wddfti %>%
anti_join(stop_words)
# Word frequency filtered
wddftif <- wddftif %>%
count(word, sort = T)
# print filtered result
wddftif %>%
write_csv(paste(fold,"wordfreqfilter.csv",sep = "/"))
############
# Bigrams
wddf_bigrams <- wddf %>%
unnest_tokens(bigram, text, token = "ngrams", n =2)
wddf_bigrams %>%
count(bigram, sort = TRUE) %>%
write_csv(paste(fold,"bigrams.csv",sep = "/"))
# Bigrams no stopwords
wddf_bigrams_sep <- wddf_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
wddf_bigrams_fil <- wddf_bigrams_sep %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
# save CSV
wddf_bigrams_fil %>%
count(word1, word2, sort = TRUE) %>%
write_csv(paste(fold,"bigramsfilter.csv",sep = "/"))
glimpse(wddf_bigrams)
bigr_tf_idf <- wddf_bigrams %>%
count(video, bigram) %>%
bind_tf_idf(bigram, video, n) %>%
arrange(desc(tf_idf)) %>%
head(N) %>%
write.csv(file = "bigramstf_idf.csv")
bigr_tf_idf <- wddf_bigrams %>%
count(video, bigram) %>%
bind_tf_idf(bigram, video, n) %>%
arrange(desc(tf_idf)) %>%
head(N) %>%
write.csv(paste(fold,"bigramsfilter.csv",sep = "/"))
# save CSV
wddf_bigrams_fil %>%
count(word1, word2, sort = TRUE) %>%
head(N)
# save CSV
wddf_bigrams_fil %>%
count(word1, word2, sort = TRUE) %>%
head(N) %>%
write_csv(paste(fold,"bigramsfilter.csv",sep = "/"))
bigr_tf_idf <- wddf_bigrams %>%
count(video, bigram) %>%
bind_tf_idf(bigram, video, n) %>%
arrange(desc(tf_idf)) %>%
head(N) %>%
write.csv(paste(fold,"bigramsfilter.csv",sep = "/"))
# save CSV
wddf_bigrams_fil %>%
count(word1, word2, sort = TRUE) %>%
head(N) %>%
write_csv(paste(fold,"bigramsfilter.csv",sep = "/"))
bigr_tf_idf <- wddf_bigrams %>%
count(video, bigram) %>%
bind_tf_idf(bigram, video, n) %>%
arrange(desc(tf_idf)) %>%
head(N) %>%
write.csv(paste(fold,"bigramstf_idf.csv",sep = "/"))
dfbifiltered %>%
count(word1, word2, sort = T) %>%
unite(col = "bigram", word1,word2, sep = " ") %>%
head(N) %>%
write(file = "bigramsfrequency.csv", sep = '\t')
dfbifiltered %>%
count(word1, word2, sort = T) %>%
unite(col = "bigram", word1,word2, sep = " ") %>%
head(N) %>%
write.csv(file = "bigramsfrequency.txt", sep = '\t')
dfbifiltered %>%
count(word1, word2, sort = T) %>%
unite(col = "bigram", word1,word2, sep = " ") %>%
head(N) %>%
write.table(file = "bigramsfrequency.txt", sep = '\t')
source('C:/Users/fcalv/Desktop/R folder/tian/Climatedream/TED/script.R', echo=TRUE)
setwd(dirname(getSourceEditorContext()$path))
source('C:/Users/fcalv/Desktop/R folder/tian/Climatedream/TED/script.R', echo=TRUE)
source('C:/Users/fcalv/Desktop/R folder/tian/Climatedream/TED/script.R', echo=TRUE)
source('C:/Users/fcalv/Desktop/R folder/tian/Climatedream/TED/script.R', echo=TRUE)
bigr_tf_idf <- dfbigrams %>%
count(speech, bigram) %>%
bind_tf_idf(bigram, speech, n) %>%
arrange(desc(tf_idf)) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
unite(col = "bigram", word1,word2, sep = " ") %>%
head(N) %>%
write.table(file = "bigramstf_idf_filtered.txt", sep = '\t')
